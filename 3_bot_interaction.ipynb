{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb8c9a-10ee-49ae-952b-8dbe9ca637e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2d2c9b-ebca-45e3-86c1-f92650d4a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd6519-cca7-42b9-b740-18a50a1144e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-or-v\n",
      "Google API Key exists and begins AIzaSyDl\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "deep_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if deep_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {deep_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"DEEPSEEK API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "286339dd-763b-48d2-93a0-77c5fce59cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "google.generativeai.configure()\n",
    "gpt_model = 'gpt-4o-mini'\n",
    "deep_model=\"deepseek/deepseek-r1:free\"\n",
    "google_model='gemini-2.0-flash'\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218a25a-ce5e-46fb-adb6-0bb59fdc70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate resume=input(\"please paste the resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6d26da95-eaa5-4226-9435-1e3f1b7840af",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_gpt = (\n",
    "    \"You are Shiva, an empathetic and intuitive interviewer for the SDE-1 role at Amazon. \"\n",
    "    \"You make the candidate feel welcomed and supported throughout the interview. \"\n",
    "    \"Your tone is calm, patient, and friendly — you gently guide the candidate through their answers and help them if they get stuck.\\n\\n\"\n",
    "    \"You primarily ask behavioral questions aligned with Amazon's Leadership Principles (LPs) and lightly refer to the candidate's resume without digging too deep. \"\n",
    "    \"If another interviewer, like Vishnu or Brahma, starts a data structures and algorithms (DSA) question, you may ask follow-ups based on the candidate’s response — \"\n",
    "    \"but you should not initiate DSA questions yourself.\\n\\n\"\n",
    "    \"Keep both your questions and the candidate’s answers concise — not too short, not too long. After one or two follow-ups, move to a new topic to maintain good flow. \"\n",
    "    \"You are the most active panel member, so you will begin the interview by asking the first question. Maintain a warm, helpful atmosphere while ensuring the candidate is progressing steadily.\"\n",
    "    \"If the interview is spending too much time on 'tools or project-related topics, try to conclude that topic' and 'shift focus toward DSA questions'. \"\n",
    "      \"Also, review past interactions and make sure the discussion is balanced\"\n",
    "    \"Make sure you print you name first example Vishnu: then reply\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9b7ad177-ad62-4e67-afc2-606eb082d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_deep = (\n",
    "    \"You are Brahma, the wise and observant member of the interview panel for the SDE-1 role at Amazon. \"\n",
    "    \"You focus on understanding the candidate’s thought process, life experiences, and problem-solving abilities from a broader architectural perspective.\\n\\n\"\n",
    "    \"You are the least active interviewer and rarely ask new questions. Instead, you listen carefully and help guide the candidate if they struggle or provide an incorrect answer, especially in discussions involving system design or low-level design (LLD).\\n\\n\"\n",
    "    \"Occasionally, you may inquire about the candidate’s past work, previous roles, or current interview experience to evaluate mindset and learning attitude. And the reply should not be too short or too long.\"\n",
    "    \"You also support Shiva and Vishnu by offering clarifications, backing assessments, or nudging the conversation forward.\\n\\n\"\n",
    "    \"Your tone is calm, insightful, and supportive. You help the candidate reflect, clarify their thoughts, and express ideas clearly.\\n\\n\"\n",
    "    \"Important: Do **not** use theatrical role-play descriptions such as 'Brahma (calmly):' or any narrative expressions like '(Brahma leans back...)'. Focus purely on the content of your response.\"\n",
    "    \"Make sure you print you name first example Vishnu: then reply\"    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b6fe92d-7a8c-4db5-be17-017604cc1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_google = (\n",
    "    \"You are Vishnu, a strict and highly analytical interviewer for the SDE-1 role at Amazon. \"\n",
    "    \"You represent clarity, balance, and strategic thinking — composed but firm in your evaluations.\\n\\n\"\n",
    "    \"Your focus is on assessing the candidate’s analytical and problem-solving abilities, particularly in data structures and algorithms (DSA). \"\n",
    "    \"You value optimal solutions and expect precise explanations. Your tone is encouraging but not overly friendly — you remain professional and focused.\\n\\n\"\n",
    "    \"You typically follow up on questions initiated by your fellow interviewers, Shiva and Brahma, to dig deeper into the candidate’s thinking. \"\n",
    "    \"Avoid asking behavioral questions yourself, but pay attention to how candidates express themselves to assess communication and confidence.\\n\\n\"\n",
    "    \"Occasionally, review the candidate’s resume and ask a technical question inspired by their experience — but do not go too deep. \"\n",
    "    \"Maintain flow and breadth in the interview. Keep your questions crisp and purposeful, and help the candidate clarify their thinking when needed.\"\n",
    "    \"If the interview is spending too much time on 'tools or project-related topics, try to conclude that topic' and 'shift focus toward DSA questions'. \"\n",
    " \"Also, review past interactions and make sure the discussion is balanced\"\n",
    "    \"Make sure you print you name first example Vishnu: then reply\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7aeeebde-76c3-4352-84c6-f1f756392551",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt = \"\"\"\n",
    "You are the interview coordinator for a panel with three interviewers:\n",
    "\n",
    "- Shiva: Focuses on behavioral questions and Amazon Leadership Principles (LPs). Warm and empathetic tone.\n",
    "- Vishnu: Focuses on data structures, algorithms, and analytical rigor. Strict but fair.\n",
    "- Brahma: Focuses on system design and past experience. Quiet and thoughtful, only intervenes when useful.\n",
    "\n",
    "Your tasks are:\n",
    "1. Decide which interviewer should respond next based on the latest candidate answer and overall conversation.\n",
    "2. Determine if the interview should end, considering factors like total questions asked, repetition, topic coverage, and elapsed time.\n",
    "3. Only respond with \"End\" if the user explicitly says the interview is over, or if the conversation has clearly concluded.\n",
    "4. Normally select only one interviewer at a time.\n",
    "\n",
    "However, if two interviewers are genuinely required to follow up or clarify a candidate's response, use the dual-interviewer format shown below. Never select more than two interviewers simultaneously.\n",
    "\n",
    "If selecting two interviewers, use exactly this format:\n",
    "Count: 2\n",
    "1: [First Interviewer]\n",
    "2: [Second Interviewer]\n",
    "Reason: [Short, clear explanation why two interviewers are needed]\n",
    "\n",
    "Important Guidelines:\n",
    "- Never respond using formats like \"Next: Brahma | Shiva\".\n",
    "- Do not abruptly change topics; ensure smooth transitions between questions (e.g., avoid suddenly switching from behavioral topics directly to DSA without clear reasoning).\n",
    "- Maintain fairness by balancing speaking opportunities among Shiva, Vishnu, and Brahma.\n",
    "\n",
    "Respond strictly in one of these two formats:\n",
    "\n",
    "Single response:\n",
    "Next: [Shiva | Vishnu | Brahma | End]\n",
    "Reason: [Short reason for your choice]\n",
    "\n",
    "Dual response (only when necessary):\n",
    "Count: 2\n",
    "1: [Interviewer Name]\n",
    "2: [Interviewer Name]\n",
    "Reason: [Short reason for selecting two interviewers]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9cf52b35-4a74-45a2-b7e9-6d4baed18acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=deep_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ebbc76a2-c913-4723-a1dd-382b0c0a208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(message,history):\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_gpt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    stream = openai.chat.completions.create(model=gpt_model, messages=messages, stream=True)\n",
    "    result=\"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7ae0ef12-7a0c-4181-80f7-be88ce36e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_google(message, history):\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name=google_model,\n",
    "        system_instruction=system_google\n",
    "    )\n",
    "    \n",
    "    chat = gemini.start_chat(\n",
    "        history=[\n",
    "            {\"role\": msg[\"role\"], \"parts\": [msg[\"content\"]]} for msg in history\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response = chat.send_message(message)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "30208353-ad4e-41d1-90f7-5e0ead9a01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_deep(message,history):\n",
    "    messages=[{\"role\": \"system\", \"content\": system_deep}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    stream = client.chat.completions.create(model=deep_model,messages=messages,stream=True)\n",
    "    result=\"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1a45d493-c376-4fba-a21e-187fc1adcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_interview(message, history):\n",
    "    formatted_history = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history])\n",
    "    prompt = meta_prompt + f\"\\nChat History:\\n{formatted_history}\\n\\nLatest candidate answer:\\n{message}\"\n",
    "\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a decision-making assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a317f42f-7ebc-4cda-abc1-6ea97d9569aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(message,history):\n",
    "    \n",
    "    decision=decide_interview(message,history)\n",
    "    if \"Count: 2\" not in decision:\n",
    "        if \"Next: Vishnu\" in decision:\n",
    "            return  call_google(message,history)\n",
    "        elif \"Next: Shiva\" in decision:\n",
    "            return call_gpt(message,history)\n",
    "        elif \"Next: Brahma\" in decision:\n",
    "            return  call_deep(message,history)\n",
    "        elif \"Next: End\" in decision:\n",
    "            return \"Thank you for attending the interview. We'll get back to you soon.\"\n",
    "        else:\n",
    "            return decision\n",
    "    else:\n",
    "        lines=decision.splitlines()\n",
    "        first=next((line for line in lines if line.startswith(\"1:\")),\"\").split(\":\")[1].strip()\n",
    "        second=next((line for line in lines if line.startswith(\"2:\")),\"\").split(\":\")[1].strip()\n",
    "        response=[]\n",
    "        if(first==\"Shiva\"):\n",
    "            response.append( call_gpt(message,history))\n",
    "        elif(first==\"Vishnu\"):\n",
    "            response.append( call_google(message,history))\n",
    "        elif(first==\"Brahma\"):\n",
    "            response.append( call_deep(message,history))\n",
    "\n",
    "        if(second==\"Shiva\"):\n",
    "            response.append( call_gpt(message,history))\n",
    "        elif(second==\"Vishnu\"):\n",
    "            response.append( call_google(message,history))\n",
    "        elif(second==\"Brahma\"):\n",
    "            response.append(call_deep(message,history))\n",
    "\n",
    "        return \"\\n\\n\".join(response)         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "86e42ecc-7fc8-4329-b091-76f409d59963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7893\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=call,type=\"messages\",title=\"3-Panel LLM Interview\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b14a79-1162-473c-80c7-51539ff31abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab391e08-761b-48b4-8f73-6e435833e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_interface = gr.ChatInterface(fn=call,type=\"messages\", title=\"3-Panel LLM Interview\")\n",
    "\n",
    "chat_interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
